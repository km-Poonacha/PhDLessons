{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 - Crawl and Scrape \n",
    "## 1. Making the request "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using 'requests' module\n",
    "Use the requests module to make a HTTP request to http://www.tripadvisor.com\n",
    "- Check the status of the request \n",
    "- Display the response header information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "url = 'http://www.tripadvisor.com/'\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response.status_code)\n",
    "#print(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get the HTML content from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "url = 'http://tripadvisor.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.status_code)\n",
    "else:\n",
    "    print('Failed to get a response from the url. Error code: ',resp.status_code )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Get the '/robots.txt' file contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "url = 'http://www.tripadvisor.com/robots.txt'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.status_code)\n",
    "    #print(response.text)\n",
    "else:\n",
    "    print('Failed to get a response from the url. Error code: ',resp.status_code )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping websites\n",
    "Sometimes, you may want a little bit of information - a movie rating, stock price, or product availability - but the information is available only in HTML pages, surrounded by ads and extraneous content.\n",
    "\n",
    "To do this we build an automated web fetcher called a crawler or spider. After the HTML contents have been retrived from the remote web servers, a scraper parses it to find the needle in the haystack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 BeautifulSoup Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BS4 module can be used for searching a webpage (HTML file) and pulling required data from it. It does three things to make a HTML page searchable-\n",
    "* First, converts the HTML page to Unicode, and HTML entities are converted to Unicode characters\n",
    "* Second, parses (analyses) the HTML page using the best available parser. It will use an HTML parser unless you specifically tell it to use an XML parser\n",
    "* Finally transforms a complex HTML document into a complex tree of Python objects.\n",
    "\n",
    "This module takes the HTML page and creates four kinds of objects: Tag, NavigableString, BeautifulSoup, and Comment.\n",
    "* The ***BeautifulSoup*** object itself represents the webpage as a whole\n",
    "* A ***Tag*** object corresponds to an XML or HTML tag in the webpage\n",
    "* The ***NavigableString*** class to contains the bit of text within a tag\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1 id=\"HEADING\" property=\"name\" class=\"heading_name   \">\n",
    "    <div class=\"heading_height\"></div>\n",
    "     \"\n",
    "     Le Jardin Napolitain\n",
    "     \"\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Making the soup\n",
    "\n",
    "First we need to use the BeautifulSoup module to parse the HTML data into Python readable Unicode Text format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let us write the code to parse a html page. We will use the trip advisor URL for an infamous restaurant - https://www.tripadvisor.com/Restaurant_Review-g187147-d1751525-Reviews-Cafe_Le_Dome-Paris_Ile_de_France.html *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "scrape_url = 'https://www.tripadvisor.com/Restaurant_Review-g187147-d1751525-Reviews-Cafe_Le_Dome-Paris_Ile_de_France.html'  \n",
    "response = requests.get(scrape_url) \n",
    "print(response.status_code)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') # Soup\n",
    "    #print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Inspect the element you want to scrape \n",
    "\n",
    "In this step we will inspect the HTML data of the website to understand the tags and attributes that matches the element. Let us inspect the HTML data of the URL and understand where (under which tag) the review data is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class=\"entry\">\n",
    "    <p class=\"partial_entry\">\n",
    "    Popped in on way to Eiffel Tower for lunch, big mistake. \n",
    "    Pizza was disgusting and service was poor. \n",
    "    It’s a shame Trip Advisor don’t let you score venues zero....\n",
    "    <span class=\"taLnk ulBlueLinks\" onclick=\"widgetEvCall('handlers.clickExpand',event,this);\">More\n",
    "    </span>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Searching the soup for the data\n",
    "\n",
    "Beautiful Soup defines a lot of methods for searching the parse tree (soup), the two most popular methods are: find() and find_all(). \n",
    "\n",
    "The simplest filter is a tag. Pass a tag to a search method and Beautiful Soup will perform a match against that exact string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let us try and find all the < p > (paragraph) tags in the soup:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrapecontent(url):\n",
    "    \"\"\"This function parses the HTML page representing the url using the BeautifulSoup module\n",
    "    and returns the created python readable data structure (soup)\"\"\"\n",
    "    scrape_response = requests.get(url) \n",
    "    print(scrape_response.status_code)\n",
    "\n",
    "    if scrape_response.status_code == 200:\n",
    "        soup = BeautifulSoup(scrape_response.text, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print('Error accessing url : ',scrape_response.status_code)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    scrape_url = 'https://www.tripadvisor.com/Restaurant_Review-g187147-d1751525-Reviews-Cafe_Le_Dome-Paris_Ile_de_France.html'  \n",
    "    ret_soup = scrapecontent(scrape_url)\n",
    "    if ret_soup:\n",
    "        for review in ret_soup.find_all('p', class_='partial_entry'):        \n",
    "            print(review.text) #We are interested only in the text data, since the reviews are stored as text\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Enable pagination\n",
    "\n",
    "Automatically access subsequent pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrapecontent(url):\n",
    "    \"\"\"This function parses the HTML page representing the url using the BeautifulSoup module\n",
    "    and returns the created python readable data structure (soup)\"\"\"\n",
    "    scrape_response = requests.get(url) \n",
    "    print(scrape_response.status_code)\n",
    "\n",
    "    if scrape_response.status_code == 200:\n",
    "        soup = BeautifulSoup(scrape_response.text, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print('Error accessing url : ',scrape_response.status_code)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    page_no = 0\n",
    "\n",
    "    while(page_no < 60):\n",
    "        scrape_url = 'https://www.tripadvisor.com/Restaurant_Review-g187147-d1751525-Reviews-or'+str(page_no)+'-Cafe_Le_Dome-Paris_Ile_de_France.html'  \n",
    "        ret_soup = scrapecontent(scrape_url)\n",
    "        if ret_soup:\n",
    "            for review in ret_soup.find_all('p', class_='partial_entry'):        \n",
    "                print(review.text) #We are interested only in the text data, since the reviews are stored as text\n",
    "        page_no = page_no + 10\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using yesterdays sentiment analysis code and the corpus of sentiment found in the word_sentiment.csv file, calculate the sentiment of the reviews.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanding this further\n",
    "\n",
    "To add additional details we can inspect the tags further and add the reviewer rating and reviwer details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def scrapecontent(url):\n",
    "    \"\"\"This function parses the HTML page representing the url using the BeautifulSoup module\n",
    "    and returns the created python readable data structure (soup)\"\"\"\n",
    "    scrape_response = requests.get(url) \n",
    "    print(scrape_response.status_code)\n",
    "\n",
    "    if scrape_response.status_code == 200:\n",
    "        soup = BeautifulSoup(scrape_response.text, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print('Error accessing url : ',scrape_response.status_code)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    scrape_url = 'https://www.tripadvisor.com/Restaurant_Review-g187147-d1751525-Reviews-Cafe_Le_Dome-Paris_Ile_de_France.html'  \n",
    "    ret_soup = scrapecontent(scrape_url)\n",
    "    if ret_soup:\n",
    "        for rev_data in ret_soup.find_all('div', class_= 'review'):\n",
    "            date = rev_data.find('span', class_ ='ratingDate')# Get the date if the review\n",
    "            print(date.text)\n",
    "            review = rev_data.find('p') # Get the review text\n",
    "            print(review.text)\n",
    "            rating = rev_data.find('span',class_='ui_bubble_rating') #Get the rating of the review\n",
    "            print(int(rating['class'][1][7:])/10)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using the review data and the ratings available is there any way we can improve the corpus of sentiments \"word_sentiment.csv\" file?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
