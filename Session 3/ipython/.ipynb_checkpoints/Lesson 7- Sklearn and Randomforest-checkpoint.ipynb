{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "* Averaging methods: Here the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. Example: Random forest\n",
    "\n",
    "* Boosting methods: Here base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble. Example: Adaboost\n",
    "\n",
    "### Random Forest classifier\n",
    "In random forests, each tree in the ensemble is built from a sample drawn with replacement from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn\n",
    "High level module built on NumPy, SciPy, and matplotlib. Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import data\n",
    "*Create dataframes from the csv data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TRAIN_CSV = \"C:\\\\Users\\kmpoo\\Dropbox\\HEC\\Teaching\\Python for PhD May 2019\\python4phd\\Session 3\\Sent\\sentence_review.csv\"\n",
    "dataframe = pd.read_csv(TRAIN_CSV, sep=\",\",error_bad_lines=False,header= 0, low_memory=False, encoding = \"Latin1\")\n",
    "print(dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Manipulate the dataframe and add new features\n",
    "*Create a new feature \"length of the sentence\"*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.assign(nWords = lambda x : x['review_text'].str.split().str.len() )\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Split into train and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle #To shuffle the dataframe\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataframe = shuffle(dataframe)\n",
    "df_train, df_test = train_test_split(dataframe, test_size=0.2)\n",
    "print(\"size of trainig data \", len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Tokenization of sentence and feature extraction\n",
    "Text data requires special preparation before you can start using it for predictive modeling.\n",
    "\n",
    "The text must be parsed to remove words, called tokenization. Then the words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n",
    "##### Convert sentences to TF-IDF vectors \n",
    "tf-idf ( term frequency–inverse document frequency), is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def vectorize(corpus, train_sentence, test_sentence):\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "                                        strip_accents='unicode',\n",
    "                                        analyzer='word',\n",
    "                                        stop_words='english',\n",
    "                                        ngram_range=(1, 3),\n",
    "                                        max_features=20000)\n",
    "\n",
    "    word_vectorizer.fit(corpus)\n",
    "    train_x = word_vectorizer.transform(train_sentence)\n",
    "    test_x = word_vectorizer.transform(test_sentence)\n",
    "    print(train_x)\n",
    "    return train_x, test_x\n",
    "\n",
    "train_x, test_x = vectorize(dataframe['review_text'],df_train['review_text'],df_test['review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5:Generate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "r = rfc.fit(train_x, df_train['sentiment'])\n",
    "acc = r.score(test_x,df_test['sentiment'])\n",
    "print(\"accuracy of rfc is = \", acc)\n",
    "rfc.predict_proba(test_x)[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Use the model to predict the sentiment of your sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series('My name is poonacha')\n",
    "s2 = pd.Series(\"his movie was absolutely horrible. A boring, random, nonsensical mess from start to finish. The film is incompetently directed from a very poor script. It feels more like a superhero movie from the early 2000's such as Catwoman or Daredevil. Watching it makes it clear that the people involved had no idea what they were doing, and should never have been put in charge of a project this size to begin with. The story makes no sense, and the whole reason Batman wants to kill Superman is contrived. Batman and Superman hate each other because they both cause collateral damage and human death, and neither one ever sees fit to point out their similarities, or try and talk to each other about their different perspectives. Apparently that would have been too interesting, so of course Snyder didn't include it.\")\n",
    "x1, x2 = vectorize(dataframe['review_text'],s1,s2)\n",
    "print(rfc.predict_proba(x2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
